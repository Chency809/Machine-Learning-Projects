{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "#import utils\n",
    "from ipynb.fs.full.utils import*\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "def load_date():\n",
    "    # Define format of the data we would like to generate\n",
    "    FORMATS = ['short',\n",
    "               'medium',\n",
    "               'long',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'full',\n",
    "               'd MMM YYY', \n",
    "               'd MMMM YYY',\n",
    "               'dd MMM YYY',\n",
    "               'd MMM, YYY',\n",
    "               'd MMMM, YYY',\n",
    "               'dd, MMM YYY',\n",
    "               'd MM YY',\n",
    "               'd MMMM YYY',\n",
    "               'MMMM d YYY',\n",
    "               'MMMM d, YYY',\n",
    "               'dd.MM.YY']\n",
    "\n",
    "    # change this if you want it to work with another language\n",
    "    LOCALES = ['en_US']\n",
    "    \n",
    "    \"\"\"\n",
    "        Loads some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
    "        human_readable = human_readable.lower()\n",
    "        human_readable = human_readable.replace(',','')\n",
    "        machine_readable = dt.isoformat()\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt\n",
    "\n",
    "def load_dataset(m):\n",
    "    \"\"\"\n",
    "        Loads a dataset with m examples and vocabularies\n",
    "        :m: the number of examples to generate\n",
    "    \"\"\"\n",
    "    \n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "    Tx = 30\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(m)):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "    \n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))    \n",
    "    inv_human = {v:k for k,v in human.items()}\n",
    "    \n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab) + ['<go>', '<eos>']))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    "     \n",
    "    return dataset, human, inv_human, machine, inv_machine\n",
    "\n",
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \n",
    "    X, Y = zip(*dataset)\n",
    "    \n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "    \n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    return X, np.array(Y), Xoh, Yoh\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "    \n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    #make lower to standardize\n",
    "    string = string.lower()\n",
    "    string = string.replace(',','')\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    \n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<pad>']] * (length - len(string))\n",
    "    \n",
    "    #print (rep)\n",
    "    return rep\n",
    "\n",
    "\n",
    "def int_to_string(ints, inv_vocab):\n",
    "    \"\"\"\n",
    "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
    "    \n",
    "    Arguments:\n",
    "    ints -- list of integers representing indexes in the machine's vocabulary\n",
    "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
    "    \n",
    "    Returns:\n",
    "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
    "    \"\"\"\n",
    "    \n",
    "    l = [inv_vocab[i] for i in ints]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(keras.Model):\n",
    "    def __init__(self, enc_v_dim, dec_v_dim, emb_dim, units, max_pred_len, start_token, end_token):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        # encoder\n",
    "        self.enc_embeddings = keras.layers.Embedding(\n",
    "            input_dim=enc_v_dim, output_dim=emb_dim,  # [enc_n_vocab, emb_dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0, 0.1))\n",
    "        \n",
    "        self.encoder = keras.layers.LSTM(units=units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # decoder\n",
    "        self.dec_embeddings = keras.layers.Embedding(\n",
    "            input_dim=dec_v_dim, output_dim=emb_dim,  # [dec_n_vocab, emb_dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0, 0.1))\n",
    "        \n",
    "        self.decoder_cell = keras.layers.LSTMCell(units=units)\n",
    "        decoder_dense = keras.layers.Dense(dec_v_dim)\n",
    "        \n",
    "        '''\n",
    "        Training decoder with TrainingSampler shares the same decoder cell(LSTM cell) and output layer(dense)\n",
    "        with predicting decoder with GreedyEmbeeding Sampler.\n",
    "        '''\n",
    "        # train decoder\n",
    "        self.decoder_train = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.TrainingSampler(), # sampler for train which read its input, and correct wrong output  \n",
    "            output_layer=decoder_dense)\n",
    "        \n",
    "        # predict decoder\n",
    "        self.decoder_eval = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(),       # sampler for predict\n",
    "            output_layer=decoder_dense\n",
    "        )\n",
    "\n",
    "        self.cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.opt = keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99, beta_2=0.999, epsilon=10**-8)\n",
    "        self.max_pred_len = max_pred_len\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def encode(self, x):\n",
    "        embedded = self.enc_embeddings(x)\n",
    "        '''\n",
    "        In LSTM, there are two initial states (hidden state a_0, and cell state), so init_s contains both.\n",
    "        '''\n",
    "        init_s = [tf.zeros((x.shape[0], self.units)), tf.zeros((x.shape[0], self.units))]\n",
    "        # outputs (all hidden state of each time step), last hidden state(a), last cell state(c)\n",
    "        o, h, c = self.encoder(embedded, initial_state=init_s) \n",
    "        return [h, c]\n",
    "\n",
    "    def inference(self, x):\n",
    "        s = self.encode(x)\n",
    "        \n",
    "        '''\n",
    "        def initialize(self, embedding, start_tokens, end_token, initial_state):\n",
    "        \"\"\"Initialize the decoder.\n",
    "        Args:\n",
    "          embedding: A `Tensor` (or `Variable`) to pass as the `params` argument\n",
    "            for `tf.nn.embedding_lookup`. This overrides `embedding_fn` set in\n",
    "            the constructor.\n",
    "          start_tokens: Start the decoding from these tokens.\n",
    "            A `int32` `Tensor` of shape `[batch_size]`.\n",
    "          end_token: The token that marks the end of decoding.\n",
    "            A `int32` scalar `Tensor`.\n",
    "          initial_state: The initial cell state as a (possibly nested) structure\n",
    "            of `Tensor` and `TensorArray`.\n",
    "        Returns:\n",
    "          `(finished, start_inputs, initial_state)`.\n",
    "          \n",
    "        1. self.dec_embeddings.variables[0] returns word matrix.\n",
    "        2. start_inputs is y_i, and initial state is S_i\n",
    "        '''\n",
    "        done, i, s = self.decoder_eval.initialize(\n",
    "            self.dec_embeddings.variables[0],\n",
    "            start_tokens=tf.fill([x.shape[0], ], self.start_token),\n",
    "            end_token=self.end_token,\n",
    "            initial_state=s)\n",
    "        \n",
    "        '''\n",
    "        def step(self, time, inputs, state, training=None):\n",
    "        Perform a decoding step.\n",
    "        Args:\n",
    "          time: scalar `int32` tensor.\n",
    "          inputs: A (structure of) input tensors.\n",
    "          state: A (structure of) state tensors and TensorArrays.\n",
    "          training: Python boolean.\n",
    "          \n",
    "        Returns:\n",
    "          `(outputs, next_state, next_inputs, finished)`.\n",
    "          \n",
    "        1. For predict output, we don't update layer's variables, so we set training=False.\n",
    "        2. Also, we use for loop to predict output at each timestep.\n",
    "        '''\n",
    "        \n",
    "        pred_id = np.zeros((x.shape[0], self.max_pred_len), dtype=np.int32)\n",
    "        for l in range(self.max_pred_len):\n",
    "            o, s, i, done = self.decoder_eval.step(\n",
    "                time=l, inputs=i, state=s, training=False)\n",
    "            pred_id[:, l] = o.sample_id\n",
    "            \n",
    "        return pred_id\n",
    "\n",
    "    def train_logits(self, x, y, seq_len):\n",
    "        s = self.encode(x)\n",
    "        \n",
    "#         print(s[0].shape)\n",
    "#         print(s[1].shape)\n",
    "        dec_in = y[:, :-1]   # ignore <EOS>\n",
    "#         print(\"dec_in:\", dec_in.shape)\n",
    "        dec_emb_in = self.dec_embeddings(dec_in)\n",
    "#         print(\"dec_emb_in:\", dec_emb_in.shape)\n",
    "        \n",
    "#         print(\"seq_len\", seq_len.shape)\n",
    "        o, _, _ = self.decoder_train(dec_emb_in, s, sequence_length=seq_len)\n",
    "        \n",
    "#         print(o.rnn_output.shape)\n",
    "        logits = o.rnn_output\n",
    "        return logits\n",
    "\n",
    "    def step(self, x, y, seq_len):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.train_logits(x, y, seq_len)\n",
    "            dec_out = y[:, 1:]  # ignore <GO>\n",
    "            \n",
    "#             print(\"x:\", x.shape)\n",
    "#             print(\"y:\", y.shape)\n",
    "#             print(\"seq_len:\", seq_len.shape)\n",
    "#             print(\"dec_out:\", dec_out.shape)\n",
    "#             print(\"logits:\", logits.shape)\n",
    "            \n",
    "            loss = self.cross_entropy(dec_out, logits)\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "            \n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sample_num, iteration):\n",
    "    # get and process data\n",
    "    m = sample_num\n",
    "    dataset, human_vocab, inv_human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "    Tx = 30\n",
    "    Ty = 10\n",
    "    X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "    Y = np.insert(Y, 0, 11, axis=1)\n",
    "    Y = np.insert(Y, Y.shape[1], 12, axis=1)\n",
    "    decoder_len = np.ones((Y.shape[0],), dtype=int)*11\n",
    "    \n",
    "    print(\"X's shape: \", X.shape)\n",
    "    print(\"X's vocab: \", len(inv_human_vocab))\n",
    "    print(\"Y's shape: \", Y.shape)\n",
    "    print(\"Y's vocab: \", len(inv_machine_vocab))\n",
    "    print(\"Decoder_len.shape: \", decoder_len.shape)\n",
    "\n",
    "    \n",
    "#     print(\"Chinese time order: yy/mm/dd \", data.date_cn[:3], \"\\nEnglish time order: dd/M/yyyy \", data.date_en[:3])\n",
    "#     print(\"vocabularies: \", data.vocab)\n",
    "#     print(\"x index sample: \\n{}\\n{}\".format(data.idx2str(data.x[0]), data.x[0]),\n",
    "#           \"\\ny index sample: \\n{}\\n{}\".format(data.idx2str(data.y[0]), data.y[0]))\n",
    "\n",
    "    \n",
    "    model = Seq2Seq(\n",
    "        37, 13, emb_dim=16, units=32,\n",
    "        max_pred_len=11, start_token=11, end_token=12)\n",
    "\n",
    "    # training\n",
    "    for t in range(iteration):\n",
    "        loss = model.step(X, Y, decoder_len)\n",
    "        \n",
    "        if t % 50 == 0:\n",
    "            index = random.randint(0, m-1)\n",
    "            target = \"\".join(int_to_string(Y[index, 1:-1], inv_machine_vocab))\n",
    "            pred = model.inference(X[index:index+1])\n",
    "            \n",
    "#             print(\"Prediciton: \", pred)\n",
    "            inf = \"\".join(int_to_string(pred[0], inv_machine_vocab))\n",
    "            src = \"\".join(int_to_string(X[index:index+1][0], inv_human_vocab)).replace('<pad>', '')\n",
    "            print(\"iteration: \", t,\n",
    "                  \"| loss: %.3f\" % loss,\n",
    "                  \"| input: \", src,\n",
    "                  \"| target: \", target,\n",
    "                  \"| inference: \", inf,)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:03<00:00, 29175.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape:  (100000, 30)\n",
      "X's vocab:  37\n",
      "Y's shape:  (100000, 12)\n",
      "Y's vocab:  13\n",
      "Decoder_len.shape:  (100000,)\n",
      "t:  0 | loss: 2.559 | input:  9/10/70 | target:  1970-09-10 | inference:  00000000000\n",
      "t:  20 | loss: 1.660 | input:  15.11.19 | target:  2019-11-15 | inference:  1990-0-01<eos><eos>\n",
      "t:  40 | loss: 1.140 | input:  january 1 2016 | target:  2017-01-01 | inference:  1997-01-19<eos>\n",
      "t:  60 | loss: 0.976 | input:  19 aug 2005 | target:  2005-08-19 | inference:  1997-01-19<eos>\n",
      "t:  80 | loss: 0.926 | input:  wednesday july 4 1990 | target:  1990-07-04 | inference:  1998-02-12<eos>\n",
      "t:  100 | loss: 0.896 | input:  tuesday january 10 1995 | target:  1995-01-10 | inference:  1975-02-25<eos>\n",
      "t:  120 | loss: 0.870 | input:  16 november 2007 | target:  2007-11-16 | inference:  1974-07-25<eos>\n",
      "t:  140 | loss: 0.840 | input:  04 oct 2003 | target:  2003-10-04 | inference:  1974-07-24<eos>\n",
      "t:  160 | loss: 0.810 | input:  07 feb 1982 | target:  1982-02-07 | inference:  1998-08-11<eos>\n",
      "t:  180 | loss: 0.787 | input:  tuesday september 27 1983 | target:  1983-09-27 | inference:  1983-05-19<eos>\n",
      "t:  200 | loss: 0.757 | input:  23 january 2009 | target:  2009-01-23 | inference:  1990-04-24<eos>\n",
      "t:  220 | loss: 0.717 | input:  02 mar 2020 | target:  2020-03-02 | inference:  1982-08-01<eos>\n",
      "t:  240 | loss: 0.671 | input:  monday may 17 1982 | target:  1982-05-17 | inference:  1980-01-11<eos>\n",
      "t:  260 | loss: 0.615 | input:  tuesday august 25 1992 | target:  1992-08-25 | inference:  1998-03-05<eos>\n",
      "t:  280 | loss: 0.558 | input:  friday march 24 2006 | target:  2006-03-24 | inference:  2006-05-04<eos>\n",
      "t:  300 | loss: 0.511 | input:  friday february 21 1992 | target:  1992-02-21 | inference:  1992-01-19<eos>\n",
      "t:  320 | loss: 0.478 | input:  saturday february 10 1973 | target:  1973-02-10 | inference:  1973-01-10<eos>\n",
      "t:  340 | loss: 0.425 | input:  monday september 24 1984 | target:  1984-09-24 | inference:  1984-10-24<eos>\n",
      "t:  360 | loss: 0.450 | input:  09 mar 1984 | target:  1984-03-09 | inference:  1984-04-08<eos>\n",
      "t:  380 | loss: 0.366 | input:  wednesday january 15 1986 | target:  1986-01-15 | inference:  1986-05-15<eos>\n",
      "t:  400 | loss: 0.318 | input:  thursday october 5 2006 | target:  2006-10-05 | inference:  2006-10-05<eos>\n",
      "t:  420 | loss: 0.310 | input:  friday august 2 2019 | target:  2019-08-02 | inference:  2019-08-29<eos>\n",
      "t:  440 | loss: 0.265 | input:  26.07.90 | target:  1990-07-26 | inference:  1990-06-06<eos>\n",
      "t:  460 | loss: 0.305 | input:  4 07 84 | target:  1984-07-04 | inference:  1984-07-04<eos>\n",
      "t:  480 | loss: 0.228 | input:  11 apr 2008 | target:  2008-04-11 | inference:  2008-01-12<eos>\n",
      "t:  500 | loss: 0.201 | input:  02.06.91 | target:  1991-06-02 | inference:  1992-03-01<eos>\n",
      "t:  520 | loss: 0.186 | input:  wednesday march 23 1994 | target:  1994-03-23 | inference:  1994-03-25<eos>\n",
      "t:  540 | loss: 0.201 | input:  10 aug 1993 | target:  1993-08-10 | inference:  1993-02-10<eos>\n"
     ]
    }
   ],
   "source": [
    "# m = 100,000, ite = 1000\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 19645.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape:  (1000, 30)\n",
      "X's vocab:  37\n",
      "Y's shape:  (1000, 12)\n",
      "Y's vocab:  13\n",
      "Decoder_len.shape:  (1000,)\n",
      "t:  0 | loss: 2.566 | input:  9 may 1998 | target:  1998-05-09 | inference:  -----------\n",
      "t:  50 | loss: 1.190 | input:  thursday march 4 1982 | target:  1982-03-04 | inference:  1998-0-01-2\n",
      "t:  100 | loss: 0.916 | input:  10/19/70 | target:  1970-10-19 | inference:  1998-02-29<eos>\n",
      "t:  150 | loss: 0.849 | input:  2 july 1974 | target:  1974-07-02 | inference:  1970-07-21<eos>\n",
      "t:  200 | loss: 0.801 | input:  28 feb 2005 | target:  2005-02-28 | inference:  2004-03-24<eos>\n",
      "t:  250 | loss: 0.752 | input:  thursday february 20 1975 | target:  1975-02-20 | inference:  1974-03-28<eos>\n",
      "t:  300 | loss: 0.698 | input:  saturday january 16 2010 | target:  2010-01-16 | inference:  2000-01-15<eos>\n",
      "t:  350 | loss: 0.645 | input:  saturday december 22 1984 | target:  1984-12-22 | inference:  1974-02-23<eos>\n",
      "t:  400 | loss: 0.588 | input:  19 january 1972 | target:  1972-01-19 | inference:  1982-01-17<eos>\n",
      "t:  450 | loss: 0.528 | input:  friday september 6 1985 | target:  1985-09-06 | inference:  1995-09-05<eos>\n",
      "t:  500 | loss: 0.465 | input:  tuesday february 16 1988 | target:  1988-02-16 | inference:  1988-12-16<eos>\n",
      "t:  550 | loss: 0.392 | input:  thursday august 21 2008 | target:  2008-08-21 | inference:  2008-07-21<eos>\n",
      "t:  600 | loss: 0.321 | input:  monday february 18 1991 | target:  1991-02-18 | inference:  1991-12-17<eos>\n",
      "t:  650 | loss: 0.261 | input:  15 sep 2004 | target:  2004-09-15 | inference:  2004-05-16<eos>\n",
      "t:  700 | loss: 0.216 | input:  tuesday june 3 2008 | target:  2008-06-03 | inference:  2008-07-31<eos>\n",
      "t:  750 | loss: 0.179 | input:  thursday december 3 1987 | target:  1987-12-03 | inference:  1987-12-31<eos>\n",
      "t:  800 | loss: 0.148 | input:  4 may 1976 | target:  1976-05-04 | inference:  1976-05-04<eos>\n",
      "t:  850 | loss: 0.120 | input:  monday february 24 2014 | target:  2014-02-24 | inference:  2014-02-24<eos>\n",
      "t:  900 | loss: 0.094 | input:  27 oct 1975 | target:  1975-10-27 | inference:  1975-10-27<eos>\n",
      "t:  950 | loss: 0.073 | input:  february 1 2009 | target:  2009-02-01 | inference:  2009-03-12<eos>\n"
     ]
    }
   ],
   "source": [
    "# m = 1000, ite = 1000\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 23599.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape:  (1000, 30)\n",
      "X's vocab:  37\n",
      "Y's shape:  (1000, 12)\n",
      "Y's vocab:  13\n",
      "Decoder_len.shape:  (1000,)\n",
      "iteration:  0 | loss: 2.571 | input:  wednesday april 25 2018 | target:  2018-04-25 | inference:  1111111-1-1\n",
      "iteration:  50 | loss: 1.231 | input:  march 21 1990 | target:  1990-03-21 | inference:  1997-01-13<eos>\n",
      "iteration:  100 | loss: 0.948 | input:  24.08.13 | target:  2013-08-24 | inference:  1970-02-18<eos>\n",
      "iteration:  150 | loss: 0.899 | input:  4 january 1994 | target:  1994-01-04 | inference:  1976-09-19<eos>\n",
      "iteration:  200 | loss: 0.870 | input:  12.07.71 | target:  1971-07-12 | inference:  1982-05-27<eos>\n",
      "iteration:  250 | loss: 0.834 | input:  tuesday june 9 2020 | target:  2020-06-09 | inference:  1991-08-27<eos>\n",
      "iteration:  300 | loss: 0.795 | input:  june 22 1993 | target:  1993-06-22 | inference:  1991-09-29<eos>\n",
      "iteration:  350 | loss: 0.751 | input:  5 september 2016 | target:  2016-09-05 | inference:  1996-11-27<eos>\n",
      "iteration:  400 | loss: 0.699 | input:  tuesday august 11 1970 | target:  1970-08-11 | inference:  1990-09-11<eos>\n",
      "iteration:  450 | loss: 0.647 | input:  tuesday april 15 2003 | target:  2003-04-15 | inference:  1993-09-15<eos>\n",
      "iteration:  500 | loss: 0.587 | input:  03 jan 1973 | target:  1973-01-03 | inference:  1973-01-13<eos>\n",
      "iteration:  550 | loss: 0.530 | input:  6 july 2017 | target:  2017-07-06 | inference:  2017-04-06<eos>\n",
      "iteration:  600 | loss: 0.467 | input:  02 mar 1999 | target:  1999-03-02 | inference:  1999-04-19<eos>\n",
      "iteration:  650 | loss: 0.422 | input:  04 may 1999 | target:  1999-05-04 | inference:  1999-05-04<eos>\n",
      "iteration:  700 | loss: 0.378 | input:  21 oct 1985 | target:  1985-10-21 | inference:  1985-10-21<eos>\n",
      "iteration:  750 | loss: 0.337 | input:  3/4/73 | target:  1973-03-04 | inference:  1973-05-04<eos>\n",
      "iteration:  800 | loss: 0.295 | input:  sep 18 1989 | target:  1989-09-18 | inference:  1989-09-28<eos>\n",
      "iteration:  850 | loss: 0.256 | input:  april 11 2010 | target:  2010-04-11 | inference:  2010-04-11<eos>\n",
      "iteration:  900 | loss: 0.222 | input:  saturday september 26 1981 | target:  1981-09-26 | inference:  1981-09-26<eos>\n",
      "iteration:  950 | loss: 0.190 | input:  26 aug 1980 | target:  1980-08-26 | inference:  1970-08-24<eos>\n",
      "iteration:  1000 | loss: 0.164 | input:  thursday january 27 2000 | target:  2000-01-27 | inference:  2000-01-27<eos>\n",
      "iteration:  1050 | loss: 0.139 | input:  wednesday february 25 1987 | target:  1987-02-25 | inference:  1987-02-15<eos>\n",
      "iteration:  1100 | loss: 0.118 | input:  october 12 2007 | target:  2007-10-12 | inference:  2007-10-21<eos>\n",
      "iteration:  1150 | loss: 0.100 | input:  03 jun 2014 | target:  2014-06-03 | inference:  2014-06-03<eos>\n",
      "iteration:  1200 | loss: 0.085 | input:  27 jul 1979 | target:  1979-07-27 | inference:  1979-07-27<eos>\n",
      "iteration:  1250 | loss: 0.073 | input:  21 august 1975 | target:  1975-08-21 | inference:  1975-08-21<eos>\n",
      "iteration:  1300 | loss: 0.063 | input:  22 12 89 | target:  1989-12-22 | inference:  1989-12-22<eos>\n",
      "iteration:  1350 | loss: 0.055 | input:  december 11 2015 | target:  2015-12-11 | inference:  2015-12-11<eos>\n",
      "iteration:  1400 | loss: 0.122 | input:  7 jun 1986 | target:  1986-06-07 | inference:  1986-06-07<eos>\n",
      "iteration:  1450 | loss: 0.069 | input:  19 march 2018 | target:  2018-03-19 | inference:  2018-03-19<eos>\n",
      "iteration:  1500 | loss: 0.049 | input:  29 september 1975 | target:  1975-09-29 | inference:  1975-09-29<eos>\n",
      "iteration:  1550 | loss: 0.042 | input:  15 nov 1999 | target:  1999-11-15 | inference:  1999-11-15<eos>\n",
      "iteration:  1600 | loss: 0.036 | input:  april 14 1992 | target:  1992-04-14 | inference:  1992-04-14<eos>\n",
      "iteration:  1650 | loss: 0.031 | input:  4 12 93 | target:  1993-12-04 | inference:  1993-12-04<eos>\n",
      "iteration:  1700 | loss: 0.027 | input:  1 apr 1976 | target:  1976-04-01 | inference:  1976-04-01<eos>\n",
      "iteration:  1750 | loss: 0.024 | input:  1/24/88 | target:  1988-01-24 | inference:  1988-01-24<eos>\n",
      "iteration:  1800 | loss: 0.021 | input:  friday september 11 1970 | target:  1970-09-11 | inference:  1970-09-11<eos>\n",
      "iteration:  1850 | loss: 0.019 | input:  friday september 28 2001 | target:  2001-09-28 | inference:  2001-09-28<eos>\n",
      "iteration:  1900 | loss: 0.017 | input:  15 oct 2017 | target:  2017-10-15 | inference:  2017-10-15<eos>\n",
      "iteration:  1950 | loss: 0.015 | input:  19 jan 2003 | target:  2003-01-19 | inference:  2003-01-19<eos>\n",
      "iteration:  2000 | loss: 0.014 | input:  9/25/12 | target:  2012-09-25 | inference:  2012-09-25<eos>\n",
      "iteration:  2050 | loss: 0.013 | input:  tuesday october 31 2000 | target:  2000-10-31 | inference:  2000-10-31<eos>\n",
      "iteration:  2100 | loss: 0.012 | input:  31.08.81 | target:  1981-08-31 | inference:  1981-08-31<eos>\n",
      "iteration:  2150 | loss: 0.011 | input:  saturday march 16 1974 | target:  1974-03-16 | inference:  1974-03-16<eos>\n",
      "iteration:  2200 | loss: 0.010 | input:  6 january 2008 | target:  2008-01-06 | inference:  2008-01-06<eos>\n",
      "iteration:  2250 | loss: 0.009 | input:  8 09 12 | target:  2012-09-08 | inference:  2012-09-08<eos>\n",
      "iteration:  2300 | loss: 0.008 | input:  tuesday november 14 1978 | target:  1978-11-14 | inference:  1978-11-14<eos>\n",
      "iteration:  2350 | loss: 0.008 | input:  30 sep 1993 | target:  1993-09-30 | inference:  1993-09-30<eos>\n",
      "iteration:  2400 | loss: 0.007 | input:  thursday november 21 1974 | target:  1974-11-21 | inference:  1974-11-21<eos>\n",
      "iteration:  2450 | loss: 0.006 | input:  30 mar 1986 | target:  1986-03-30 | inference:  1986-03-30<eos>\n",
      "iteration:  2500 | loss: 0.006 | input:  sep 19 1991 | target:  1991-09-19 | inference:  1991-09-19<eos>\n",
      "iteration:  2550 | loss: 0.005 | input:  4/13/09 | target:  2009-04-13 | inference:  2009-04-13<eos>\n",
      "iteration:  2600 | loss: 0.005 | input:  wednesday august 8 2018 | target:  2018-08-08 | inference:  2018-08-08<eos>\n",
      "iteration:  2650 | loss: 0.005 | input:  thursday may 12 1983 | target:  1983-05-12 | inference:  1983-05-12<eos>\n",
      "iteration:  2700 | loss: 0.004 | input:  21.09.95 | target:  1995-09-21 | inference:  1995-09-21<eos>\n",
      "iteration:  2750 | loss: 0.004 | input:  thursday may 28 1981 | target:  1981-05-28 | inference:  1981-05-28<eos>\n",
      "iteration:  2800 | loss: 0.004 | input:  19.06.70 | target:  1970-06-19 | inference:  1970-06-19<eos>\n",
      "iteration:  2850 | loss: 0.004 | input:  sunday july 7 1991 | target:  1991-07-07 | inference:  1991-07-07<eos>\n",
      "iteration:  2900 | loss: 0.003 | input:  sunday november 3 2019 | target:  2019-11-03 | inference:  2019-11-03<eos>\n",
      "iteration:  2950 | loss: 0.003 | input:  monday january 17 1994 | target:  1994-01-17 | inference:  1994-01-17<eos>\n",
      "iteration:  3000 | loss: 0.003 | input:  28 apr 2010 | target:  2010-04-28 | inference:  2010-04-28<eos>\n",
      "iteration:  3050 | loss: 0.003 | input:  friday march 25 1988 | target:  1988-03-25 | inference:  1988-03-25<eos>\n",
      "iteration:  3100 | loss: 0.003 | input:  thursday december 28 2006 | target:  2006-12-28 | inference:  2006-12-28<eos>\n",
      "iteration:  3150 | loss: 0.002 | input:  24 08 92 | target:  1992-08-24 | inference:  1992-08-24<eos>\n",
      "iteration:  3200 | loss: 0.002 | input:  saturday december 3 1977 | target:  1977-12-03 | inference:  1977-12-03<eos>\n",
      "iteration:  3250 | loss: 0.002 | input:  12 jan 2012 | target:  2012-01-12 | inference:  2012-01-12<eos>\n",
      "iteration:  3300 | loss: 0.002 | input:  3 march 2011 | target:  2011-03-03 | inference:  2011-03-03<eos>\n",
      "iteration:  3350 | loss: 0.002 | input:  june 5 1991 | target:  1991-06-05 | inference:  1991-06-05<eos>\n",
      "iteration:  3400 | loss: 0.002 | input:  4/23/88 | target:  1988-04-23 | inference:  1988-04-23<eos>\n",
      "iteration:  3450 | loss: 0.002 | input:  19 apr 1987 | target:  1987-04-19 | inference:  1987-04-19<eos>\n",
      "iteration:  3500 | loss: 0.002 | input:  9 oct 2010 | target:  2010-10-09 | inference:  2010-10-09<eos>\n",
      "iteration:  3550 | loss: 0.002 | input:  30.12.73 | target:  1973-12-30 | inference:  1973-12-30<eos>\n",
      "iteration:  3600 | loss: 0.001 | input:  friday october 29 1999 | target:  1999-10-29 | inference:  1999-10-29<eos>\n",
      "iteration:  3650 | loss: 0.001 | input:  january 4 1989 | target:  1989-01-04 | inference:  1989-01-04<eos>\n",
      "iteration:  3700 | loss: 0.001 | input:  03 aug 1986 | target:  1986-08-03 | inference:  1986-08-03<eos>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  3750 | loss: 0.001 | input:  6 april 2014 | target:  2014-04-06 | inference:  2014-04-06<eos>\n",
      "iteration:  3800 | loss: 0.001 | input:  8 dec 1979 | target:  1979-12-08 | inference:  1979-12-08<eos>\n",
      "iteration:  3850 | loss: 0.001 | input:  3 october 1997 | target:  1997-10-03 | inference:  1997-10-03<eos>\n",
      "iteration:  3900 | loss: 0.001 | input:  30 mar 1986 | target:  1986-03-30 | inference:  1986-03-30<eos>\n",
      "iteration:  3950 | loss: 0.001 | input:  thursday march 12 1998 | target:  1998-03-12 | inference:  1998-03-12<eos>\n",
      "iteration:  4000 | loss: 0.001 | input:  friday september 21 2012 | target:  2012-09-21 | inference:  2012-09-21<eos>\n",
      "iteration:  4050 | loss: 0.001 | input:  thursday february 28 1985 | target:  1985-02-28 | inference:  1985-02-28<eos>\n",
      "iteration:  4100 | loss: 0.001 | input:  sunday july 8 1984 | target:  1984-07-08 | inference:  1984-07-08<eos>\n",
      "iteration:  4150 | loss: 0.001 | input:  sunday january 28 1990 | target:  1990-01-28 | inference:  1990-01-28<eos>\n",
      "iteration:  4200 | loss: 0.001 | input:  thursday may 4 2000 | target:  2000-05-04 | inference:  2000-05-04<eos>\n",
      "iteration:  4250 | loss: 0.001 | input:  21 02 81 | target:  1981-02-21 | inference:  1981-02-21<eos>\n",
      "iteration:  4300 | loss: 0.001 | input:  4/2/04 | target:  2004-04-02 | inference:  2004-04-02<eos>\n",
      "iteration:  4350 | loss: 0.001 | input:  tuesday june 14 1988 | target:  1988-06-14 | inference:  1988-06-14<eos>\n",
      "iteration:  4400 | loss: 0.001 | input:  saturday december 5 1992 | target:  1992-12-05 | inference:  1992-12-05<eos>\n",
      "iteration:  4450 | loss: 0.001 | input:  tuesday august 14 2012 | target:  2012-08-14 | inference:  2012-08-14<eos>\n",
      "iteration:  4500 | loss: 0.001 | input:  20 october 2018 | target:  2018-10-20 | inference:  2018-10-20<eos>\n",
      "iteration:  4550 | loss: 0.001 | input:  9 april 1986 | target:  1986-04-09 | inference:  1986-04-09<eos>\n",
      "iteration:  4600 | loss: 0.001 | input:  april 7 2019 | target:  2019-04-07 | inference:  2019-04-07<eos>\n",
      "iteration:  4650 | loss: 0.001 | input:  wednesday may 13 1987 | target:  1987-05-13 | inference:  1987-05-13<eos>\n",
      "iteration:  4700 | loss: 0.001 | input:  sunday may 11 1980 | target:  1980-05-11 | inference:  1980-05-11<eos>\n",
      "iteration:  4750 | loss: 0.001 | input:  2 july 2012 | target:  2012-07-02 | inference:  2012-07-02<eos>\n",
      "iteration:  4800 | loss: 0.001 | input:  sunday march 8 2020 | target:  2020-03-08 | inference:  2020-03-08<eos>\n",
      "iteration:  4850 | loss: 0.001 | input:  saturday september 27 2008 | target:  2008-09-27 | inference:  2008-09-27<eos>\n",
      "iteration:  4900 | loss: 0.000 | input:  14 march 1980 | target:  1980-03-14 | inference:  1980-03-14<eos>\n",
      "iteration:  4950 | loss: 0.000 | input:  4/13/09 | target:  2009-04-13 | inference:  2009-04-13<eos>\n"
     ]
    }
   ],
   "source": [
    "# m = 1000, ite = 5000\n",
    "model = train(1000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, inv_human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "Y = np.insert(Y, 0, 11, axis=1)\n",
    "Y = np.insert(Y, Y.shape[1], 12, axis=1)\n",
    "decoder_len = np.ones((Y.shape[0],), dtype=int)*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| input:  friday november 23 1979 | inference:  1979-11-23<eos>\n"
     ]
    }
   ],
   "source": [
    "t = 9998\n",
    "pred = model.inference(X[t:t+1])\n",
    "\n",
    "inf = \"\".join(int_to_string(pred[0], inv_machine_vocab))\n",
    "src = \"\".join(int_to_string(X[t:t+1][0], inv_human_vocab)).replace('<pad>', '')\n",
    "print(\n",
    "  \"| input: \", src,\n",
    "  \"| inference: \", inf,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[t:t+1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=inputs.reshape(1,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array(string_to_int('July 9 2021', 30, human_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| input:  july 9 2021 | inference:  2012-05-07<eos>\n"
     ]
    }
   ],
   "source": [
    "t = 9998\n",
    "pred = model.inference(inputs)\n",
    "\n",
    "inf = \"\".join(int_to_string(pred[0], inv_machine_vocab))\n",
    "src = \"\".join(int_to_string(inputs[0], inv_human_vocab)).replace('<pad>', '')\n",
    "print(\n",
    "  \"| input: \", src,\n",
    "  \"| inference: \", inf,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5',\n",
       " '/',\n",
       " '1',\n",
       " '0',\n",
       " '/',\n",
       " '7',\n",
       " '0',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_string(X[0:1][0], inv_human_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
